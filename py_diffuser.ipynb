{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laserine32/serba-sd/blob/main/py_diffuser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiJDJX4EIjHl"
      },
      "source": [
        "# Step 1Ô∏è‚É£ : Install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "kXZ32E19Fh9a"
      },
      "outputs": [],
      "source": [
        "#@title ## <img src=\"https://ssl.gstatic.com/images/branding/product/1x/drive_2020q4_48dp.png\" height=\"20px\"> Mount GDrive\n",
        "#@markdown Is used to store generation results. the results of the generation will be saved on `SD-IMG-OUT/img`\n",
        "from IPython.display import clear_output\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "clear_output()\n",
        "flag_gfpgan_installed = False\n",
        "print(\"‚úÖ Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4O3v9vFRGErJ"
      },
      "outputs": [],
      "source": [
        "#@title ## ‚öôÔ∏è Install\n",
        "#@markdown Install Dependcy\n",
        "!pip install xformers transformers omegaconf safetensors accelerate compel\n",
        "\n",
        "!pip install diffusers\n",
        "!pip install scipy ftfy\n",
        "\n",
        "\n",
        "!wget https://huggingface.co/runwayml/stable-diffusion-v1-5/raw/main/v1-inference.yaml\n",
        "!wget https://raw.githubusercontent.com/huggingface/diffusers/main/scripts/convert_original_stable_diffusion_to_diffusers.py\n",
        "\n",
        "clear_output()\n",
        "print(\"‚úÖ Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## üè∞ Install GFPGAN\n",
        "#@markdown used for facial restoration and enhancement. If you don't want to install you can skip it and install later if you need it.\n",
        "%cd /content\n",
        "!rm -rf GFPGAN\n",
        "!git clone https://github.com/TencentARC/GFPGAN.git\n",
        "%cd GFPGAN\n",
        "\n",
        "# Set up the environment\n",
        "# Install basicsr - https://github.com/xinntao/BasicSR\n",
        "# We use BasicSR for both training and inference\n",
        "!pip install basicsr\n",
        "# Install facexlib - https://github.com/xinntao/facexlib\n",
        "# We use face detection and face restoration helper in the facexlib package\n",
        "!pip install facexlib\n",
        "# Install other depencencies\n",
        "!pip install -r requirements.txt\n",
        "!python setup.py develop\n",
        "!pip install realesrgan  # used for enhancing the background (non-face) regions\n",
        "\n",
        "!wget https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth -P experiments/pretrained_models\n",
        "!wget https://github.com/TencentARC/GFPGAN/releases/download/v0.2.0/GFPGANCleanv1-NoCE-C2.pth -P experiments/pretrained_models\n",
        "!wget https://github.com/TencentARC/GFPGAN/releases/download/v0.1.0/GFPGANv1.pth -P experiments/pretrained_models\n",
        "!wget https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.4.pth -P experiments/pretrained_models\n",
        "\n",
        "%cd /content\n",
        "clear_output()\n",
        "flag_gfpgan_installed = True\n",
        "print(\"‚úÖ Done!\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "8demp8S77uWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "OsdFbiui7GIR"
      },
      "outputs": [],
      "source": [
        "#@title ## ‚öôÔ∏è Init\n",
        "#@markdown initialize the program defining pipes and functions\n",
        "import diffusers\n",
        "import transformers\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import shutil\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from diffusers import StableDiffusionPipeline\n",
        "\n",
        "from PIL import Image\n",
        "import hashlib\n",
        "import datetime\n",
        "import random\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device_name = torch.device(\"cuda\")\n",
        "    torch_dtype = torch.float16\n",
        "else:\n",
        "    device_name = torch.device(\"cpu\")\n",
        "    torch_dtype = torch.float32\n",
        "\n",
        "# Plot pipeline outputs.\n",
        "def plot_images(images, labels = None):\n",
        "    N = len(images)\n",
        "    n_cols = 5\n",
        "    n_rows = int(np.ceil(N / n_cols))\n",
        "\n",
        "    plt.figure(figsize = (20, 5 * n_rows))\n",
        "    for i in range(len(images)):\n",
        "        plt.subplot(n_rows, n_cols, i + 1)\n",
        "        if labels is not None:\n",
        "            plt.title(labels[i])\n",
        "        plt.imshow(np.array(images[i]))\n",
        "        plt.axis(False)\n",
        "    plt.show()\n",
        "\n",
        "def get_row_col(data):\n",
        "  col = 6\n",
        "  row = 1\n",
        "  if len(data) > 6:\n",
        "    len_data = len(data)\n",
        "    row = 2\n",
        "    if len_data > 12:\n",
        "      row = len_data // col + 1\n",
        "    if len_data % col == 0:\n",
        "      row = len_data // col\n",
        "  return col, row\n",
        "\n",
        "def image_grid(imgs):\n",
        "    cols, rows = get_row_col(imgs)\n",
        "    w, h = imgs[0].size\n",
        "    aspect_ratio = h / w\n",
        "    w = 256\n",
        "    h = int(w * aspect_ratio)\n",
        "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
        "    grid_w, grid_h = grid.size\n",
        "\n",
        "    for i, img in enumerate(imgs):\n",
        "        hmi = img.resize((w, h))\n",
        "        grid.paste(hmi, box=(i%cols*w, i//cols*h))\n",
        "    return grid\n",
        "\n",
        "# Prompt embeddings to overcome CLIP 77 token limit.\n",
        "# https://github.com/huggingface/diffusers/issues/2136\n",
        "\n",
        "def get_prompt_embeddings(\n",
        "    pipe,\n",
        "    prompt,\n",
        "    negative_prompt,\n",
        "    split_character = \",\",\n",
        "    device = torch.device(\"cpu\")\n",
        "):\n",
        "    max_length = pipe.tokenizer.model_max_length\n",
        "    # Simple method of checking if the prompt is longer than the negative\n",
        "    # prompt - split the input strings using `split_character`.\n",
        "    count_prompt = len(prompt.split(split_character))\n",
        "    count_negative_prompt = len(negative_prompt.split(split_character))\n",
        "\n",
        "    # If prompt is longer than negative prompt.\n",
        "    if count_prompt >= count_negative_prompt:\n",
        "        input_ids = pipe.tokenizer(\n",
        "            prompt, return_tensors = \"pt\", truncation = False\n",
        "        ).input_ids.to(device)\n",
        "        shape_max_length = input_ids.shape[-1]\n",
        "        negative_ids = pipe.tokenizer(\n",
        "            negative_prompt,\n",
        "            truncation = False,\n",
        "            padding = \"max_length\",\n",
        "            max_length = shape_max_length,\n",
        "            return_tensors = \"pt\"\n",
        "        ).input_ids.to(device)\n",
        "\n",
        "    # If negative prompt is longer than prompt.\n",
        "    else:\n",
        "        negative_ids = pipe.tokenizer(\n",
        "            negative_prompt, return_tensors = \"pt\", truncation = False\n",
        "        ).input_ids.to(device)\n",
        "        shape_max_length = negative_ids.shape[-1]\n",
        "        input_ids = pipe.tokenizer(\n",
        "            prompt,\n",
        "            return_tensors = \"pt\",\n",
        "            truncation = False,\n",
        "            padding = \"max_length\",\n",
        "            max_length = shape_max_length\n",
        "        ).input_ids.to(device)\n",
        "\n",
        "    # Concatenate the individual prompt embeddings.\n",
        "    concat_embeds = []\n",
        "    neg_embeds = []\n",
        "    for i in range(0, shape_max_length, max_length):\n",
        "        concat_embeds.append(\n",
        "            pipe.text_encoder(input_ids[:, i: i + max_length])[0]\n",
        "        )\n",
        "        neg_embeds.append(\n",
        "            pipe.text_encoder(negative_ids[:, i: i + max_length])[0]\n",
        "        )\n",
        "\n",
        "    return torch.cat(concat_embeds, dim = 1), torch.cat(neg_embeds, dim = 1)\n",
        "\n",
        "def restore_face(out_dir, fn, model_version, scl):\n",
        "  source_img = \"/content/GFPGAN/tmp_img/\"+fn+\".png\"\n",
        "  tmp_out = \"/content/GFPGAN/tmp_out/\"\n",
        "  os.makedirs(tmp_out, exist_ok=True)\n",
        "  !python /content/GFPGAN/inference_gfpgan.py -i $source_img -o $tmp_out -v $model_version -s $scl --bg_upsampler realesrgan\n",
        "  frm_img = f\"{tmp_out}restored_imgs/{fn}.png\"\n",
        "  !mv $frm_img $out_dir\n",
        "  return Image.open(out_dir+\"/\"+fn+\".png\")\n",
        "\n",
        "\n",
        "clear_output()\n",
        "print(\"‚úÖ Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwqiHQGwH6W1"
      },
      "source": [
        "# Step 2Ô∏è‚É£ : üì• Download Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "DmdWlG21H3yN"
      },
      "outputs": [],
      "source": [
        "#@title ## From : ü§ó Huggingface\n",
        "\n",
        "#@markdown You need to accept the model license before downloading or using the Stable Diffusion weights. Please, visit the [model card](https://huggingface.co/runwayml/stable-diffusion-v1-5), read the license and tick the checkbox if you agree. You have to be a registered user in ü§ó Hugging Face Hub, and you'll also need to use an access token for the code to work.\n",
        "# https://huggingface.co/settings/tokens\n",
        "!mkdir -p ~/.huggingface\n",
        "HUGGINGFACE_TOKEN = \"\" #@param {type:\"string\"}\n",
        "!echo -n \"{HUGGINGFACE_TOKEN}\" > ~/.huggingface/token\n",
        "\n",
        "#@markdown #### <a name=\"diffusionmodel\"><font size=\"4\" color=\"#e8cf53\">**Diffusion Model**:</font></a>\n",
        "MODEL_ID = 'runwayml/stable-diffusion-v1-5' #@param [\"runwayml/stable-diffusion-v1-5\", \"runwayml/stable-diffusion-inpainting\", \"CompVis/stable-diffusion-v1-4-original\", \"CompVis/stable-diffusion-v1-3\",\"CompVis/stable-diffusion-v1-2\",\"CompVis/stable-diffusion-v1-1\",\"SG161222/Realistic_Vision_V5.1_noVAE\",\"hakurei/waifu-diffusion\",\"nitrosocke/redshift-diffusion\",\"lambdalabs/sd-pokemon-diffusers\",\"doohickey/trinart-waifu-diffusion-50-50\",\"spav/nilou-waifu-diffusion\",\"valhalla/sd-wikiart-v2\",\"rrustom/stable-architecture-diffusers\",\"AstraliteHeart/pony-diffusion\",\"nitrosocke/redshift-diffusion\",\"nitrosocke/Arcane-Diffusion\",\"prompthero/openjourney\", \"nitrosocke/Nitro-Diffusion\", \"\"]{allow-input: true}\n",
        "# markdown <font size=\"3\">Use the drop-down arrow to select a model, or enter in a custom model from Hugging Face.</font><br>\n",
        "# markdown <font size=\"3\">The model `nitrosocke/redshift-diffusion` does not support `LOW_VRAM_PATCH` mode.</font><br>\n",
        "# markdown <font size=\"3\">If using `CACHE_PIPELINES` you will need to run `RECACHE_PIPES` once when switching `MODEL_ID`. Allows custom input (for HF models not listed)</font>\n",
        "\n",
        "clear_output()\n",
        "print(\"‚úÖ Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "GRsUmxP7gX72"
      },
      "outputs": [],
      "source": [
        "#@title ## From : üß© Other\n",
        "#@markdown #### Select only one of the methods below to download the model\n",
        "#@markdown #### <font color=\"#FFFF00\">Note: this program can only run **one model per runtime**, so if you want to change the model you have to **restart the runtime**.\n",
        "from IPython.display import clear_output\n",
        "import os\n",
        "import sys\n",
        "import requests\n",
        "from urllib.parse import unquote\n",
        "import time\n",
        "import threading\n",
        "# from clint.textui import progress\n",
        "\n",
        "model_filename = \"\"\n",
        "\n",
        "def humansize_speed(nbytes):\n",
        "    suffixes = ['bps', 'Kbps', 'Mbps', 'Gbps', 'Tbps', 'Pbps']\n",
        "    i = 0\n",
        "    while nbytes >= 1024 and i < len(suffixes)-1:\n",
        "        nbytes /= 1024.\n",
        "        i += 1\n",
        "    f = ('%.2f' % nbytes).rstrip('0').rstrip('.')\n",
        "    return '%s %s' % (f, suffixes[i])\n",
        "\n",
        "def humansize(nbytes):\n",
        "    suffixes = ['B', 'KB', 'MB', 'GB', 'TB', 'PB']\n",
        "    i = 0\n",
        "    while nbytes >= 1024 and i < len(suffixes)-1:\n",
        "        nbytes /= 1024.\n",
        "        i += 1\n",
        "    f = ('%.2f' % nbytes).rstrip('0').rstrip('.')\n",
        "    return '%s %s' % (f, suffixes[i])\n",
        "\n",
        "def format_duration(duration):\n",
        "    mapping = [\n",
        "        ('s', 60),\n",
        "        ('m', 60),\n",
        "        ('h', 24),\n",
        "    ]\n",
        "    duration = int(duration)\n",
        "    result = []\n",
        "    for symbol, max_amount in mapping:\n",
        "        amount = duration % max_amount\n",
        "        result.append(f'{amount}{symbol}')\n",
        "        duration //= max_amount\n",
        "        if duration == 0:\n",
        "            break\n",
        "\n",
        "    if duration:\n",
        "        result.append(f'{duration}d')\n",
        "\n",
        "    return ' '.join(reversed(result))\n",
        "\n",
        "def download_model(DirectLink_URL):\n",
        "    if not DirectLink_URL:\n",
        "        print(\"Skipping download as no valid Type or DirectLink_URL provided.\")\n",
        "        return None\n",
        "\n",
        "    output_path = \"/content/custom/Stable-diffusion/\"\n",
        "    # /content/ui/embeddings\n",
        "\n",
        "    # Create the directories if they don't exist\n",
        "    os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "    urls = [url.strip() for url in DirectLink_URL.split(\",\")]\n",
        "    # download_asinkron(urls, output_path)\n",
        "    url = \"\"\n",
        "    if \"drive.google.com\" in url:\n",
        "      print(\"Doesn't support yet!\")\n",
        "      return None\n",
        "    else:\n",
        "      url = DirectLink_URL\n",
        "\n",
        "    print(\"[GET] \"+url)\n",
        "    response = requests.get(url, stream=True)\n",
        "    if response.status_code == 200:\n",
        "      content_disposition = response.headers.get('content-disposition')\n",
        "      total_length = response.headers.get('content-length')\n",
        "      if content_disposition:\n",
        "        filename = unquote(content_disposition.split('filename=')[1])\n",
        "      else:\n",
        "        filename = unquote(url.split(\"/\")[-1])  # Extracting the filename from the URL\n",
        "      # Remove double quotes and semicolons from the filename\n",
        "      filename = filename.replace('\"', '').replace(';', '')\n",
        "      filename = os.path.join(output_path, filename)  # Modify the filename to include the output path\n",
        "\n",
        "      if not os.path.exists(filename):\n",
        "        chunk_size = 5242880  # 5 MB\n",
        "        with open(filename, 'wb') as f:\n",
        "          dl = 0\n",
        "          start = time.perf_counter()\n",
        "          total_length = int(total_length)\n",
        "          for chunk in response.iter_content(chunk_size=chunk_size):\n",
        "            if chunk:\n",
        "              dl += len(chunk)\n",
        "              f.write(chunk)\n",
        "              done = int(50 * dl / total_length)\n",
        "              tmm = format_duration(time.perf_counter() - start)\n",
        "              kecepatan = humansize_speed(dl//(time.perf_counter() - start))\n",
        "              progres = humansize(dl)+\" / \"+humansize(total_length)\n",
        "              sys.stdout.write(\"\\r[%s%s] %s | %s | %s\" % ('=' * done, ' ' * (50-done), progres, kecepatan, tmm))\n",
        "              sys.stdout.flush()\n",
        "        print(\"\")\n",
        "        print(\"‚úÖ Done\", \"success\", filename)\n",
        "        return filename\n",
        "      else:\n",
        "        print(\"File already exists:\", filename)\n",
        "        return filename\n",
        "\n",
        "def download_drive(dl, pt):\n",
        "  if dl == \"\":\n",
        "    print(\"Drive link empty\")\n",
        "    return False\n",
        "  if pt != \"\":\n",
        "    %cd \"$pt\"\n",
        "  ld = \"https://drive.google.com/uc?id=\"+dl.split(\"/\")[5]\n",
        "  print(\"Downloading file: \"+ld)\n",
        "  !gdown -q $ld\n",
        "  print(\"‚úÖ Done\", \"success\", dl)\n",
        "  %cd \"/content/\"\n",
        "  return True\n",
        "\n",
        "def load_model_fromfile(path_name):\n",
        "  get_fname = path_name.split(\"/\")[-1]\n",
        "  get_ext = get_fname.split(\".\")[-1]\n",
        "  dump_path = \"/content/custom/\"+get_fname.split(\".\")[0]\n",
        "  safetensors_param = \"--from_safetensors\" if get_ext==\"safetensors\" else \"\"\n",
        "  if os.path.exists(dump_path):\n",
        "    print(\"DONE\")\n",
        "    return dump_path\n",
        "  !python convert_original_stable_diffusion_to_diffusers.py \\\n",
        "    --checkpoint_path $path_name \\\n",
        "    --dump_path $dump_path \\\n",
        "    --original_config_file /content/v1-inference.yaml \\\n",
        "    $safetensors_param\n",
        "  return dump_path\n",
        "\n",
        "opp = \"/content/custom/Stable-diffusion/\"\n",
        "os.makedirs(opp, exist_ok=True)\n",
        "\n",
        "MODEL_ID = \"\"\n",
        "mod_name = None\n",
        "#@markdown ---\n",
        "#@markdown # 1. Direct Download\n",
        "#@markdown If you use a model provider such as [Civitai](https://civitai.com/), [Tensor.Art](https://tensor.art/), etc., then copy the direct download link then paste it\n",
        "# Call the function with the first set of parameters\n",
        "DirectLink_URL = \"\"  # @param {'type': 'string'}\n",
        "if DirectLink_URL != \"\":\n",
        "  mod_name = download_model(DirectLink_URL)\n",
        "\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown # 2. GDrive Download\n",
        "#@markdown If you want to download a model from another gdrive, then apart from inputting the `link` you also have to input the `file name`.\n",
        "GDRIVE_LINK = \"\" #@param {type:\"string\"}\n",
        "FILE_NAME = \"\" #@param {type:\"string\"}\n",
        "if GDRIVE_LINK != \"\" and FILE_NAME != \"\":\n",
        "  if download_drive(GDRIVE_LINK, opp):\n",
        "    mod_name = os.path.join(opp, FILE_NAME)\n",
        "#@markdown ---\n",
        "#@markdown # 3. Local GDrive\n",
        "#@markdown load model from local gdrive.\n",
        "GDRIVE_PATH = \"\" #@param {type:\"string\"}\n",
        "if GDRIVE_PATH != \"\":\n",
        "  mod_name = GDRIVE_PATH\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "if mod_name != None:\n",
        "  MODEL_ID = load_model_fromfile(mod_name)\n",
        "  # clear_output()\n",
        "  print(MODEL_ID)\n",
        "  print(\"‚úÖ Done!\")\n",
        "else:\n",
        "  print(\"‚ùå Something went wrong!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWTEC2Ti-fq-",
        "outputId": "1f7f1ddd-bc21-4503-c698-ddea1b6b505b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Done!\n"
          ]
        }
      ],
      "source": [
        "#@title ## LoRA Downloader\n",
        "\n",
        "def download_lora(Type, DirectLink_URL):\n",
        "    if Type == \"None\" or not DirectLink_URL:\n",
        "        print(\"Skipping download as no valid Type or DirectLink_URL provided.\")\n",
        "        return\n",
        "\n",
        "    output_path = \"/content/custom/\"\n",
        "    # /content/ui/embeddings\n",
        "    if Type == \"LoRA\":\n",
        "        output_path += \"Lora/\"\n",
        "    else:\n",
        "        print(\"Invalid type specified.\")\n",
        "        return\n",
        "\n",
        "    # Create the directories if they don't exist\n",
        "    os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "    urls = [url.strip() for url in DirectLink_URL.split(\",\")]\n",
        "    download_asinkron(urls, output_path)\n",
        "    # if Type == \"Checkpoint\":\n",
        "      # download_asinkron(urls, output_path)\n",
        "    # else:\n",
        "      # download_sinkron(urls, output_path)\n",
        "\n",
        "def download_asinkron(urls, output_path):\n",
        "  tali = []\n",
        "  print(\"-- Create Thread\")\n",
        "  for url in urls:\n",
        "    if \"drive.google.com\" in url:\n",
        "      t = threading.Thread(target=download_drive, args=(url,output_path))\n",
        "    else:\n",
        "      t = threading.Thread(target=download_response, args=(url,output_path))\n",
        "    tali.append(t)\n",
        "  print(\"-- Start Thread\")\n",
        "  for t in tali:\n",
        "    t.start()\n",
        "  for t in tali:\n",
        "    t.join()\n",
        "\n",
        "  print(\"‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ\")\n",
        "  print(\"-- All Download Done! --\")\n",
        "  print(\"‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ\")\n",
        "\n",
        "def download_sinkron(urls, output_path):\n",
        "  for url in urls:\n",
        "    if \"drive.google.com\" in url:\n",
        "      download_drive(url, output_path)\n",
        "    else:\n",
        "      download_response(url, output_path)\n",
        "\n",
        "      # break\n",
        "\n",
        "def download_response(url, output_path):\n",
        "    if url!=\"\":\n",
        "        print(\"[GET] \"+url)\n",
        "        response = requests.get(url, stream=True)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            content_disposition = response.headers.get('content-disposition')\n",
        "            total_length = response.headers.get('content-length')\n",
        "            if content_disposition:\n",
        "                filename = unquote(content_disposition.split('filename=')[1])\n",
        "            else:\n",
        "                filename = unquote(url.split(\"/\")[-1])  # Extracting the filename from the URL\n",
        "\n",
        "            # Remove double quotes and semicolons from the filename\n",
        "            filename = filename.replace('\"', '').replace(';', '')\n",
        "\n",
        "            filename = os.path.join(output_path, filename)  # Modify the filename to include the output path\n",
        "\n",
        "            if not os.path.exists(filename):\n",
        "                # print(\"Downloading file:\", filename)\n",
        "                chunk_size = 5242880  # 5 MB\n",
        "                with open(filename, 'wb') as f:\n",
        "                    dl = 0\n",
        "                    start = time.perf_counter()\n",
        "                    total_length = int(total_length)\n",
        "                    for chunk in response.iter_content(chunk_size=chunk_size):\n",
        "                    # for chunk in progress.bar(response.iter_content(chunk_size=chunk_size), expected_size=(total_length/1024) + 1):\n",
        "                        if chunk:\n",
        "                            dl += len(chunk)\n",
        "                            f.write(chunk)\n",
        "                            done = int(50 * dl / total_length)\n",
        "                            tmm = format_duration(time.perf_counter() - start)\n",
        "                            kecepatan = humansize_speed(dl//(time.perf_counter() - start))\n",
        "                            progres = humansize(dl)+\" / \"+humansize(total_length)\n",
        "                            # sys.stdout.write(\"\\r[%s%s] %s | %s | %s\" % ('=' * done, ' ' * (50-done), progres, kecepatan, tmm))\n",
        "                            # sys.stdout.flush()\n",
        "                            # f.flush()\n",
        "\n",
        "                # print(\"File downloaded successfully.\")\n",
        "                print(\"‚úÖ Done\", \"success\", filename)\n",
        "            else:\n",
        "                print(\"File already exists:\", filename)\n",
        "        else:\n",
        "            print(\"Failed to download the file:\", url)\n",
        "\n",
        "    # Clear the output to keep the notebook clean\n",
        "    # clear_output()\n",
        "\n",
        "    # Print the success message\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown Downloader\n",
        "#@markdown ---\n",
        "# Call the function with the first set of parameters\n",
        "Type1 = \"LoRA\"\n",
        "DirectLink_URL = \"\"  # @param {'type': 'string'}\n",
        "if Type1 != \"None\":\n",
        "  print(\"========================================================\")\n",
        "  print(\"Download \"+Type1)\n",
        "  print(\"========================================================\")\n",
        "  download_lora(Type1, DirectLink_URL)\n",
        "  print(\"========================================================\")\n",
        "\n",
        "clear_output()\n",
        "print(\"‚úÖ Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNWVOpx0-uaZ"
      },
      "source": [
        "# Step 3Ô∏è‚É£ : Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "uj8AmVkC94Hc"
      },
      "outputs": [],
      "source": [
        "#@markdown Run this for first time running or change setting.\n",
        "from diffusers import DPMSolverMultistepScheduler\n",
        "\n",
        "# Clip skip = 2.\n",
        "# https://github.com/huggingface/diffusers/issues/3212\n",
        "\n",
        "clip_skip = 1 #@param {type:\"slider\", min:1, max:2, step:1}\n",
        "\n",
        "if clip_skip > 1:\n",
        "    text_encoder = transformers.CLIPTextModel.from_pretrained(\n",
        "        \"runwayml/stable-diffusion-v1-5\",\n",
        "        subfolder = \"text_encoder\",\n",
        "        num_hidden_layers = 12 - (clip_skip - 1),\n",
        "        torch_dtype = torch_dtype\n",
        "    )\n",
        "if clip_skip > 1:\n",
        "    pipe = diffusers.DiffusionPipeline.from_pretrained(\n",
        "        MODEL_ID,\n",
        "        torch_dtype = torch_dtype,\n",
        "        safety_checker = None,\n",
        "        text_encoder = text_encoder,\n",
        "    )\n",
        "else:\n",
        "    pipe = diffusers.DiffusionPipeline.from_pretrained(\n",
        "        MODEL_ID,\n",
        "        torch_dtype = torch_dtype,\n",
        "        safety_checker = None\n",
        "    )\n",
        "pipe = pipe.to(device_name)\n",
        "\n",
        "\n",
        "scheduler = \"Euler A\" # @param [\"Euler A\", \"Euler\", \"DPM\", \"LMS\", \"DDIM\", \"PNDM\", \"DDPM\"]\n",
        "if scheduler == \"Euler A\":\n",
        "  pipe.scheduler = diffusers.EulerAncestralDiscreteScheduler.from_config(pipe.scheduler.config)\n",
        "elif scheduler == \"DPM\":\n",
        "  pipe.scheduler = diffusers.DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
        "elif scheduler == \"LMS\":\n",
        "  pipe.scheduler = diffusers.LMSDiscreteScheduler.from_config(pipe.scheduler.config)\n",
        "elif scheduler == \"DDIM\":\n",
        "  pipe.scheduler = diffusers.DDIMScheduler.from_config(pipe.scheduler.config)\n",
        "elif scheduler == \"Euler\":\n",
        "  pipe.scheduler = diffusers.EulerDiscreteScheduler.from_config(pipe.scheduler.config)\n",
        "elif scheduler == \"PNDM\":\n",
        "  pipe.scheduler = diffusers.PNDMScheduler.from_config(pipe.scheduler.config)\n",
        "else: # DDPM\n",
        "  pipe.scheduler = diffusers.DDPMScheduler.from_config(pipe.scheduler.config)\n",
        "\n",
        "IS_LORA = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "HjW_PrvlGbpX"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@markdown ---\n",
        "\n",
        "\n",
        "# MODEL_ID = \"/content/custom/realisticVisionV51_v51VAE\"\n",
        "LORA_PATH = \"\" #@param {type:\"string\"}\n",
        "\n",
        "if LORA_PATH!= \"\":\n",
        "  if not IS_LORA:\n",
        "    pipe.load_lora_weights(LORA_PATH)\n",
        "    IS_LORA = True\n",
        "else:\n",
        "  if IS_LORA:\n",
        "    pipe.unload_lora_weights()\n",
        "    IS_LORA = False\n",
        "#@markdown - <font size=\"2\" color=\"#5bc0de\">if you using LoRA Downloader the file will be in `custom/LoRA`. Right click the LoRA file then **copy path** and then paste it.</font>\n",
        "\n",
        "\n",
        "use_prompt_embeddings = True #@param {type:\"boolean\"}\n",
        "#@markdown - <font size=\"2\" color=\"#5bc0de\">Used for preprocessing prompts.</font>\n",
        "\n",
        "prompt = \"\" #@param {type:\"string\"}\n",
        "negative_prompt = \"\" #@param {type:\"string\"}\n",
        "#@markdown - <font size=\"2\" color=\"#5bc0de\">You can't use (thing:1.2) (other thing:0.7) in diffusers. Is not exactly equals, but you use thing++ other thing--- instead.</font>\n",
        "\n",
        "\n",
        "prompt_embeds, negative_prompt_embeds = get_prompt_embeddings(\n",
        "    pipe,\n",
        "    prompt,\n",
        "    negative_prompt,\n",
        "    split_character = \",\",\n",
        "    device = device_name\n",
        ")\n",
        "\n",
        "num_inference_steps = 30 #@param {type:\"slider\", min:10, max:150, step:1}\n",
        "guidance_scale = 7 #@param {type:\"slider\", min:1, max:30, step:0.5}\n",
        "seeds =  -1 #@param {type:\"integer\"}\n",
        "#@markdown - <font size=\"2\" color=\"#5bc0de\">Seed for image generation, leave -1 to use a random seed</font>\n",
        "batch_size = 1 #@param {type:\"integer\"}\n",
        "batch_seed = -1  #@param {type:\"integer\"}\n",
        "#@markdown - <font size=\"2\" color=\"#5bc0de\">Seed for batch image generation, leave -1 to use a random seed for every generated images.</font>\n",
        "#@markdown - <font size=\"2\" color=\"#5bc0de\">If you provide a value then it will be used to increment the seed initialization.</font>\n",
        "#@markdown - <font size=\"2\" color=\"#5bc0de\">For example, if `seed = 1` `batch_size = 10` and `batch_seed = 2` then the seeds produced for each image are `[1, 3, 5, 7, 9, 11, 13, 15, 17, 19]`.</font>\n",
        "\n",
        "if seeds <= 0:\n",
        "  seeds = [random.randrange(10, 999999999)]\n",
        "else:\n",
        "  seeds = [seeds]\n",
        "if batch_size > 1:\n",
        "  if batch_seed <= 0:\n",
        "    seeds = []\n",
        "    for s in range(0, batch_size):\n",
        "      seeds.append(random.randrange(10, 999999999))\n",
        "  else:\n",
        "    seeds = [i for i in range(seeds[0] , seeds[0] + (batch_size * batch_seed) - 1, batch_seed)]\n",
        "\n",
        "width  = 512 #@param {type:\"number\"}\n",
        "height = 768 #@param {type:\"number\"}\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "use_gfpgan = False #@param {type:\"boolean\"}\n",
        "#@markdown - <font size=\"2\" color=\"#5bc0de\">**GFPGAN** is used for facial restoration and enhancement and also for Upscaling using **RealERSEGAN**.</font>\n",
        "gfpgan_model_version = \"1.4\" #@param [\"1\", \"1.2\", \"1.3\", \"1.4\"]\n",
        "x_upscale = 1 # @param {type:\"slider\", min:1, max:4, step:0.5}\n",
        "\n",
        "if not flag_gfpgan_installed:\n",
        "  use_gfpgan = False\n",
        "\n",
        "# use_prompt_embeddings = True\n",
        "xnow = datetime.datetime.now()\n",
        "timt = xnow.strftime(\"%Y-%m-%d\")\n",
        "im_save = f\"/content/drive/MyDrive/SD-IMG-OUT/img/{timt}/\"\n",
        "txt_save = f\"/content/drive/MyDrive/SD-IMG-OUT/text/{timt}/\"\n",
        "im_gfpgan = f\"/content/GFPGAN/tmp_img/\"\n",
        "os.makedirs(im_save, exist_ok=True)\n",
        "os.makedirs(txt_save, exist_ok=True)\n",
        "os.makedirs(im_gfpgan, exist_ok=True)\n",
        "\n",
        "images = []\n",
        "\n",
        "for count, seed in enumerate(seeds):\n",
        "    start_time = time.time()\n",
        "    if use_prompt_embeddings is False:\n",
        "        print(\"Generate \"+str(count+1)+\" of \"+str(len(seeds))+\" NOT USE Text Embedding\")\n",
        "        new_img = pipe(\n",
        "            prompt = prompt,\n",
        "            negative_prompt = negative_prompt,\n",
        "            width = width,\n",
        "            height = height,\n",
        "            guidance_scale = guidance_scale,\n",
        "            num_inference_steps = num_inference_steps,\n",
        "            num_images_per_prompt = 1,\n",
        "            generator = torch.manual_seed(seed),\n",
        "        ).images\n",
        "    else:\n",
        "        print(\"Generate \"+str(count+1)+\" of \"+str(len(seeds))+\" USE Text Embedding\")\n",
        "        new_img = pipe(\n",
        "            prompt_embeds = prompt_embeds,\n",
        "            negative_prompt_embeds = negative_prompt_embeds,\n",
        "            width = width,\n",
        "            height = height,\n",
        "            guidance_scale = guidance_scale,\n",
        "            num_inference_steps = num_inference_steps,\n",
        "            num_images_per_prompt = 1,\n",
        "            generator = torch.manual_seed(seed),\n",
        "        ).images\n",
        "    txtsave = prompt+\"\\n\"+\"Negative Prompt:\"+negative_prompt+\"\\n\"\n",
        "    txtsave += \"steps:\"+str(num_inference_steps)+\",cfg_scale:\"+str(guidance_scale)\n",
        "\n",
        "    x = datetime.datetime.now()\n",
        "    tmt = x.strftime(\"%Y%m%d%H%M%S\")\n",
        "    MID = MODEL_ID.split(\"/\")[-1]\n",
        "    fname_file = f\"{MID}_{tmt}_{seed}\"\n",
        "    with open(f\"{txt_save}{fname_file}.txt\", \"w\") as f:\n",
        "      f.write(txtsave)\n",
        "    if use_gfpgan:\n",
        "      new_img[0].save(f\"{im_gfpgan}{fname_file}.png\")\n",
        "      # new_img[0].save(f\"{im_save}{fname_file}.png\")\n",
        "      new_img[0] = restore_face(im_save, fname_file, gfpgan_model_version, x_upscale)\n",
        "    else:\n",
        "      new_img[0].save(f\"{im_save}{fname_file}.png\")\n",
        "    images = images + new_img\n",
        "\n",
        "clear_output()\n",
        "print(\"Displaying Images\")\n",
        "gr = image_grid(images)\n",
        "display(gr)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üêõ Debug"
      ],
      "metadata": {
        "id": "wvQvmBilcJvG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe.unload_lora_weights()"
      ],
      "metadata": {
        "id": "fQ70OXqJjCzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://drive.google.com/file/d/1C0o6rOzw-EjBh19_a5ZvnJgZ2-fCsw_R/view?usp=drive_link\"\n",
        "output_path = \"/content/custom/Stable-diffusion/\"\n",
        "download_drive(url, output_path)\n",
        "%cd /content/\n",
        "mod_name = \"/content/custom/Stable-diffusion/AZZSFASDL.ckpt\"\n",
        "MODEL_ID = load_model_fromfile(mod_name)"
      ],
      "metadata": {
        "id": "kweOHwhTHdKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Batch (Alpha)\n",
        "#@markdown this not finished yet.\n",
        "use_prompt_embeddings = True\n",
        "prompt = \"\"\"A beautiful woman, detailed, HDR, 8K, high resolution\"\"\"\n",
        "\n",
        "negative_prompt = \"\"\"deformed, weird, bad resolution, bad depiction,weird, worst quality, worst resolution,too blurry, not relevant\"\"\"\n",
        "\n",
        "\n",
        "prompt_embeds, negative_prompt_embeds = get_prompt_embeddings(\n",
        "    pipe,\n",
        "    prompt,\n",
        "    negative_prompt,\n",
        "    split_character = \",\",\n",
        "    device = device_name\n",
        ")\n",
        "\n",
        "num_inference_steps = 30\n",
        "guidance_scale = 7\n",
        "\n",
        "width  = 512\n",
        "height = 768\n",
        "use_gfpgan = True\n",
        "gfpgan_model_version = \"1.3\"\n",
        "\n",
        "list_lora = [\"/content/custom/Lora/LoRa_V3.safetensors\",\n",
        "             \"/content/custom/Lora/LoRa_V2.safetensors\",\n",
        "             \"/content/custom/Lora/LoRa_V1.safetensors\",\n",
        "             \"/content/custom/Lora/LoRa_V0.safetensors\"]\n",
        "jumlah_generate = 1\n",
        "images = []\n",
        "for lc, LORA_PATH in enumerate(list_lora):\n",
        "  pipe.load_lora_weights(LORA_PATH)\n",
        "  fnl = LORA_PATH.split(\"/\")[-1]\n",
        "  fnl = fnl.split(\".\")[0]\n",
        "  for count in range(0,jumlah_generate):\n",
        "    # print(count)\n",
        "    seed = random.randrange(10, 999999999)\n",
        "    not_print = \"\" if use_prompt_embeddings else \"NOT\"\n",
        "    print(\"Lora \"+str(lc+1)+\" of \"+str(len(list_lora))+\" Generate - \"+str(count+1)+\" of \"+str(jumlah_generate)+\" \"+not_print+\" USE Text Embedding\")\n",
        "    if use_prompt_embeddings is False:\n",
        "      new_img = pipe(\n",
        "            prompt = prompt,\n",
        "            negative_prompt = negative_prompt,\n",
        "            width = width,\n",
        "            height = height,\n",
        "            guidance_scale = guidance_scale,\n",
        "            num_inference_steps = num_inference_steps,\n",
        "            num_images_per_prompt = 1,\n",
        "            generator = torch.manual_seed(seed),\n",
        "        ).images\n",
        "    else:\n",
        "      new_img = pipe(\n",
        "            prompt_embeds = prompt_embeds,\n",
        "            negative_prompt_embeds = negative_prompt_embeds,\n",
        "            width = width,\n",
        "            height = height,\n",
        "            guidance_scale = guidance_scale,\n",
        "            num_inference_steps = num_inference_steps,\n",
        "            num_images_per_prompt = 1,\n",
        "            generator = torch.manual_seed(seed),\n",
        "        ).images\n",
        "    txtsave = prompt+\"\\n\"+\"Negative Prompt:\"+negative_prompt+\"\\n\"\n",
        "    txtsave += \"steps:\"+str(num_inference_steps)+\",cfg_scale:\"+str(guidance_scale)\n",
        "\n",
        "    x = datetime.datetime.now()\n",
        "    tmt = x.strftime(\"%Y%m%d%H%M%S\")\n",
        "    MID = MODEL_ID.split(\"/\")[-1]\n",
        "    fname_file = f\"{fnl}_{tmt}_{seed}\"\n",
        "    with open(f\"{txt_save}{fname_file}.txt\", \"w\") as f:\n",
        "      f.write(txtsave)\n",
        "    if use_gfpgan:\n",
        "      new_img[0].save(f\"{im_gfpgan}{fname_file}.png\")\n",
        "      # new_img[0].save(f\"{im_save}{fname_file}.png\")\n",
        "      new_img[0] = restore_face(im_save, fname_file, gfpgan_model_version)\n",
        "    else:\n",
        "      new_img[0].save(f\"{im_save}{fname_file}.png\")\n",
        "    images = images + new_img\n",
        "  pipe.unload_lora_weights()\n",
        "\n",
        "clear_output()\n",
        "print(\"Displaying Images\")\n",
        "gr = image_grid(images)\n",
        "display(gr)"
      ],
      "metadata": {
        "id": "RRZJXpiAY9JI",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "obj = os.scandir(im_save)\n",
        "num = 0\n",
        "for entry in obj :\n",
        "  if entry.is_file():\n",
        "    name = entry.name\n",
        "    num += 1\n",
        "print(\"You have generated \"+str(num)+\" images.\")"
      ],
      "metadata": {
        "id": "1qcfVl_flDI-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "wvQvmBilcJvG"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP7faeB3kRKtkxSPIS24OU3",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}